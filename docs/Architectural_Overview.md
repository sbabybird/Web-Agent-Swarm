# 架构概览: Web Agent Swarm

## 1. 核心理念: 一个多能力智能体平台

本项目已从一个简单的网页自动化工具，演进为一个复杂精密的、多能力的智能体平台。其核心原则不再是执行一组固定的命令，而是赋能一个由大型语言模型（LLM）构成的层级体系，使其能够理解多样化的用户目标，并生成可执行的代码来达成这些目标。

系统构建于一个 **“管理者/专家” (Manager/Expert)** 模型之上：

1.  **管理者智能体 (任务调度总管):** 这个顶层智能体负责分析用户的自然语言目标，以判断其根本*意图*。它不关心*如何*实现目标，而是关心这是*何种类型*的任务。

2.  **专家智能体 (领域专家):** 这些是专门化的智能体，每一个都通过一个详细的提示词（Prompt），在特定的领域内受过“训练”。它们从管理者那里接收用户目标，并负责生成完成任务所需的高质量、可执行的代码。

3.  **执行器服务 (运行时环境):** 这些是安全的环境，负责运行由专家们生成的代码。

这种解耦的架构具有高度的可扩展性，允许未来轻松地加入新的“专家”智能体和“执行器”服务。

---

## 2. 系统组件

### 2.1. 前端 (`React + TypeScript + Vite`)

The frontend serves as the primary user interface and the orchestration hub.

*   **`App.tsx`**: The main component that manages state, user input, and the overall workflow.
*   **`components/DrawingCanvas.tsx`**: A specialized React component that houses an HTML `<canvas>`. It is responsible for safely executing JavaScript code generated by the *Drawing Expert* using the Canvas 2D API.
*   **`prompts/`**: This directory externalizes all LLM prompts from the application logic, making them easy to maintain and modify. It contains:
    *   `manager_prompt.txt`: Instructs the Manager Agent on how to classify tasks.
    *   `drawing_prompt.txt`: Instructs the Drawing Expert on how to generate Canvas API code.

### 2.2. 后端 (`Node.js + Express`)

The backend acts as a secure gateway for capabilities that cannot be run directly in the browser.

*   **`server.js`**: The main Express server.
*   **`/run-llm`**: A proxy endpoint that securely forwards prompts to the specified LLM service (e.g., Ollama), shielding the frontend from needing direct access.
*   **`/log-error`**: A utility endpoint for the frontend to report JavaScript execution errors, aiding in debugging LLM-generated code.

### 2.3. 部署 (`Docker + Docker Compose`)

The entire system is containerized for portability and ease of setup.

*   **`docker-compose.yml`**: Orchestrates the `frontend` and `backend` services.
*   **`Dockerfile`**: Separate Dockerfiles for each service define their respective environments.
*   **`.dockerignore`**: Crucially, these files prevent the local `node_modules` directories from being copied into the Docker image during the build process, avoiding common permission and path issues, especially on Windows.
*   **Volume Strategy**: The configuration uses a hybrid approach to balance performance and developer experience. Source code is mounted directly for live-reloading, while `node_modules` are handled by high-performance named volumes within the Docker engine. The local `node_modules` (installed manually by the developer) are used only for IDE intellisense.

---

## 3. 工作流与数据流

对于一个典型的用户请求，其端到端的数据流如下：

1.  **用户输入**: 用户在UI中输入一个目标（例如：“画一个时钟”）并点击“执行”。

2.  **管理者调用**: `App.tsx` 使用用户目标填充`manager_prompt.txt`模板，并将其发送到`/run-llm`端点。

3.  **任务分类**: LLM（扮演管理者角色）分析目标，并返回一个JSON对象来对任务进行分类，例如：`{ "taskType": "drawing" }`。

4.  **专家调用**: `App.tsx`接收到这个分类结果。然后，它会选择相应的专家提示词（例如`drawing_prompt.txt`），用用户目标填充它，并再次发送到`/run-llm`端点。

5.  **代码生成**: LLM（现在扮演专家角色，如绘画专家）生成完成任务所需的可执行JavaScript代码。

6.  **执行**: `App.tsx`接收到生成的代码。
    *   如果`taskType`是`drawing`，代码会被作为prop传递给`DrawingCanvas.tsx`组件，并在前端安全地执行。

7.  **反馈**: 执行过程中的结果和日志，都会被显示在UI中，为用户提供一个完整的反馈闭环。
